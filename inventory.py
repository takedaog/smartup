import warnings
import pandas as pd
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from sqlalchemy import create_engine, text
from sqlalchemy.types import Float, Integer, String, DateTime, Boolean, NVARCHAR
import urllib
from sqlalchemy import inspect

# üîá –≥–ª—É—à–∏–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è pandas –æ —Ñ–æ—Ä–º–∞—Ç–µ –¥–∞—Ç
warnings.filterwarnings(
    "ignore",
    message="Could not infer format",
    category=UserWarning
)

def get_cookies_from_browser(url: str) -> dict:
    chrome_options = Options()
    chrome_options.add_argument("--start-maximized")
    driver = webdriver.Chrome(options=chrome_options)
    driver.get(url)
    input("üåê –ê–≤—Ç–æ—Ä–∏–∑—É–π—Ç–µ—Å—å –≤ –±—Ä–∞—É–∑–µ—Ä–µ –∏ –Ω–∞–∂–º–∏—Ç–µ Enter...")
    cookies = {c['name']: c['value'] for c in driver.get_cookies()}
    driver.quit()
    return cookies

def auto_cast_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """–ê–≤—Ç–æ–∫–∞—Å—Ç–∏–Ω–≥ + –æ—Ç–±—Ä–∞—Å—ã–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –Ω–µ–≤–∞–ª–∏–¥–Ω–æ–π –¥–∞—Ç–æ–π."""
    for col in df.columns:
        s = df[col].dropna().astype(str)
        try:
            if not s.empty and s.str.lower().isin(['true','false']).all():
                df[col] = s.str.lower().map({'true':1,'false':0}).astype('Int64')
                continue
            if not s.empty and s.str.fullmatch(r"\d+").all():
                df[col] = pd.to_numeric(s, downcast='integer', errors='coerce')
                continue
            if not s.empty and s.str.fullmatch(r"\d+\.\d+").all():
                df[col] = pd.to_numeric(s, errors='coerce')
                continue
            # –¥–∞—Ç—ã ‚Äî –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ —É–±–∏—Ä–∞–µ–º
            dt = pd.to_datetime(df[col], errors='coerce', dayfirst=True)
            if dt.notna().any():
                df[col] = dt
                df = df[dt.notna()]  # ‚ö° –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ
        except Exception:
            pass
    return df

def fetch_inventory(data_url: str, cookies: dict) -> dict:
    print("‚¨áÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ inventory...")
    r = requests.post(
        data_url,
        cookies=cookies,
        json={},
        headers={"Content-Type": "application/json"}
    )
    r.raise_for_status()
    data = r.json()
    items = data.get("inventory", [])
    if not items:
        print("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
        return {}

    inv_df = pd.json_normalize(items, sep="_", max_level=1)
    groups_list, kinds_list, sectors_list = [], [], []
    for it in items:
        pid = it.get("product_id")
        for g in it.get("groups", []):
            g["product_id"] = pid
            groups_list.append(g)
        for k in it.get("inventory_kinds", []):
            k["product_id"] = pid
            kinds_list.append(k)
        for s in it.get("sector_codes", []):
            s["product_id"] = pid
            sectors_list.append(s)

    return {
        "inventory_main":   inv_df,
        "inventory_groups": pd.DataFrame(groups_list),
        "inventory_kinds":  pd.DataFrame(kinds_list),
        "inventory_sectors":pd.DataFrame(sectors_list)
    }

UNIQUE_KEYS = {
    "inventory_main":   ["product_id"],
    "inventory_groups": ["product_id", "group_id"],
    "inventory_kinds":  ["product_id", "kind_id"],
    "inventory_sectors":["product_id", "sector_code"],
}

def upload_to_sql(df_dict: dict):
    params = urllib.parse.quote_plus(
        "DRIVER={ODBC Driver 17 for SQL Server};"
        "SERVER=localhost;"
        "DATABASE=Epco;"
        "Trusted_Connection=yes;"
        "TrustServerCertificate=yes;"
    )
    engine = create_engine(f"mssql+pyodbc:///?odbc_connect={params}")
    inspector = inspect(engine)

    with engine.begin() as conn:
        for table_name, df in df_dict.items():
            if df is None or df.empty:
                print(f"‚è≠ {table_name} –ø—É—Å—Ç ‚Äì –ø—Ä–æ–ø—É—â–µ–Ω–æ.")
                continue

            df = auto_cast_dataframe(df)
            keys = [k for k in UNIQUE_KEYS.get(table_name, []) if k in df.columns]
            if keys:
                df[keys] = df[keys].fillna('').astype(str).apply(lambda x: x.str.strip())
                df = df.drop_duplicates(subset=keys)

            # –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ NVARCHAR –¥–ª—è –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π
            dtype_map = {}
            for col in df.columns:
                val = df[col].dropna().iloc[0] if not df[col].dropna().empty else None
                if val is None: dtype_map[col] = NVARCHAR()   # ‚Üê —é–Ω–∏–∫–æ–¥
                elif isinstance(val, float): dtype_map[col] = Float()
                elif isinstance(val, int): dtype_map[col] = Integer()
                elif isinstance(val, bool): dtype_map[col] = Boolean()
                elif hasattr(val, "year"): dtype_map[col] = DateTime()
                else: dtype_map[col] = NVARCHAR()             # ‚Üê —é–Ω–∏–∫–æ–¥

            if not inspector.has_table(table_name):
                df.to_sql(table_name, con=conn, index=False,
                          if_exists="replace", dtype=dtype_map)
                print(f"üÜï {table_name} —Å–æ–∑–¥–∞–Ω–∞ –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫.")
                continue

            if keys:
                stg = f"{table_name}_stg"
                df.to_sql(stg, con=conn, index=False, if_exists="replace", dtype=dtype_map)
                on_clause = " AND ".join([f"t.[{k}] = s.[{k}]" for k in keys])
                cols = [f"[{c}]" for c in df.columns]
                insert_cols = ", ".join(cols)
                insert_vals = ", ".join([f"s.[{c}]" for c in df.columns])
                merge_sql = f"""
MERGE INTO dbo.{table_name} AS t
USING dbo.{stg} AS s
ON {on_clause}
WHEN NOT MATCHED BY TARGET THEN
    INSERT ({insert_cols}) VALUES ({insert_vals});
DROP TABLE dbo.{stg};
"""
                conn.execute(text(merge_sql))
                print(f"üì• {table_name} ‚Üí –¥–æ–±–∞–≤–ª–µ–Ω—ã —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ ({len(df)} –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ).")
            else:
                df.to_sql(table_name, con=conn, index=False,
                          if_exists="append", dtype=dtype_map)
                print(f"‚ö†Ô∏è {table_name}: –Ω–µ—Ç –∫–ª—é—á–µ–π, –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤—Å–µ {len(df)} —Å—Ç—Ä–æ–∫.")

if __name__ == "__main__":
    DATA_URL = "https://smartup.online/b/anor/mxsx/mr/inventory$export"
    cookies = get_cookies_from_browser("https://smartup.online")
    result = fetch_inventory(DATA_URL, cookies)
    if result:
        upload_to_sql(result)
