import warnings
import pandas as pd
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from sqlalchemy import create_engine, text, inspect
from sqlalchemy.types import Float, Integer, String, DateTime, Boolean, NVARCHAR
import math
import urllib

pd.set_option('future.no_silent_downcasting', True)
warnings.filterwarnings(
    "ignore",
    message="Could not infer format",
    category=UserWarning
)

def get_cookies_from_browser(url: str) -> dict:
    chrome_options = Options()
    chrome_options.add_argument("--start-maximized")
    driver = webdriver.Chrome(options=chrome_options)
    driver.get(url)
    input("üåê –ê–≤—Ç–æ—Ä–∏–∑—É–π—Ç–µ—Å—å –≤ –±—Ä–∞—É–∑–µ—Ä–µ –∏ –Ω–∞–∂–º–∏—Ç–µ Enter...")
    cookies = {c['name']: c['value'] for c in driver.get_cookies()}
    driver.quit()
    return cookies

def auto_cast_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    df = df.copy()
    for col in df.columns:
        # —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —á–∏—Å–ª–∞ –±–µ–∑ errors="ignore"
        try:
            df[col] = pd.to_numeric(df[col], downcast="integer")
        except (ValueError, TypeError):
            pass
        # –ø–æ—Ç–æ–º –¥–∞—Ç—ã
        if pd.api.types.is_object_dtype(df[col]):
            try:
                parsed = pd.to_datetime(df[col], errors="coerce", dayfirst=True)
                if parsed.notna().mean() >= 0.6:
                    df[col] = parsed
            except Exception:
                pass
    return df



def fetch_inventory(data_url: str, cookies: dict) -> dict:
    print("‚¨áÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ inventory...")
    r = requests.post(data_url, cookies=cookies, json={}, headers={"Content-Type": "application/json"})
    r.raise_for_status()
    data = r.json()
    items = data.get("inventory", [])
    print("üì° –í—Å–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ inventory (raw):", len(items))
    if not items:
        return {}
    inv_df = pd.json_normalize(items, sep="_", max_level=1)
    print("üîé –ü–æ—Å–ª–µ json_normalize:", inv_df.shape)

    groups_list, kinds_list, sectors_list = [], [], []
    for it in items:
        pid = it.get("product_id")
        for g in it.get("groups", []):
            g["product_id"] = pid
            groups_list.append(g)
        for k in it.get("inventory_kinds", []):
            k["product_id"] = pid
            kinds_list.append(k)
        for s in it.get("sector_codes", []):
            s["product_id"] = pid
            sectors_list.append(s)

    return {
        "inventory_main":   inv_df,
        "inventory_groups": pd.DataFrame(groups_list),
        "inventory_kinds":  pd.DataFrame(kinds_list),
        "inventory_sectors":pd.DataFrame(sectors_list)
    }

UNIQUE_KEYS = {
    "inventory_main":   ["product_id"],
    "inventory_groups": ["product_id", "group_id"],
    "inventory_kinds":  ["product_id", "kind_id"],
    "inventory_sectors":["product_id", "sector_code"],
}

def build_dtype_map(df: pd.DataFrame, table_name: str) -> dict:
    """
    ‚Ä¢ bool ‚Üí Boolean
    ‚Ä¢ int  ‚Üí Integer
    ‚Ä¢ float ‚Üí Float
    ‚Ä¢ datetime ‚Üí DateTime
    ‚Ä¢ object/—Å—Ç—Ä–æ–∫–∏ ‚Üí String(n) (—Å —Ä–∞—Å—á—ë—Ç–æ–º –¥–ª–∏–Ω—ã)
    ‚Ä¢ inventory_groups.group_code / type_code ‚Üí NVARCHAR
    """
    dtype_map: dict[str, object] = {}

    for col in df.columns:
        s = df[col].dropna()
        if s.empty:
            # –ü—É—Å—Ç–∞—è –∫–æ–ª–æ–Ω–∫–∞ ‚Äî —Ö–æ—Ç—è –±—ã String(50)
            dtype_map[col] = String(50)
            continue

        # –°–ø–µ—Ü-–ø—Ä–∞–≤–∏–ª–æ
        if table_name == "inventory_groups" and col in ("group_code", "type_code"):
            dtype_map[col] = NVARCHAR()
            continue

        # pandas dtype
        dt = df[col].dtype

        if pd.api.types.is_bool_dtype(dt):
            dtype_map[col] = Boolean()
        elif pd.api.types.is_integer_dtype(dt):
            # Int64/Int32 ‚Üí Integer
            dtype_map[col] = Integer()
        elif pd.api.types.is_float_dtype(dt):
            dtype_map[col] = Float()
        elif pd.api.types.is_datetime64_any_dtype(dt):
            dtype_map[col] = DateTime()
        else:
            # —Ç–µ–∫—Å—Ç: –≤—ã—á–∏—Å–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É, –Ω–µ–±–æ–ª—å—à–æ–π –∑–∞–ø–∞—Å
            max_len = s.astype(str).str.len().max()
            size = max(50, math.ceil(max_len * 1.2))
            dtype_map[col] = String(size)

    return dtype_map

def upload_to_sql(df_dict: dict):
    params = urllib.parse.quote_plus(
        "DRIVER={ODBC Driver 17 for SQL Server};"
        "SERVER=localhost;"
        "DATABASE=Epco;"
        "Trusted_Connection=yes;"
        "TrustServerCertificate=yes;"
    )
    engine = create_engine(f"mssql+pyodbc:///?odbc_connect={params}")
    inspector = inspect(engine)

    with engine.begin() as conn:
        for table_name, df in df_dict.items():
            if df is None or df.empty:
                print(f"‚è≠ {table_name} –ø—É—Å—Ç ‚Äì –ø—Ä–æ–ø—É—â–µ–Ω–æ.")
                continue

            # –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
            df = auto_cast_dataframe(df)

            keys = [k for k in UNIQUE_KEYS.get(table_name, []) if k in df.columns]

            # üîë —Å–Ω–∞—á–∞–ª–∞ —Å—Ç—Ä–æ–∏–º –∫–∞—Ä—Ç—É —Ç–∏–ø–æ–≤ ‚Äì –ø–æ–∫–∞ –∫–ª—é—á–∏ –µ—â—ë –≤ ¬´—Ä–æ–¥–Ω–æ–º¬ª dtype
            dtype_map = build_dtype_map(df, table_name)

            if keys:
                # —á–∏—Å—Ç–∏–º –ø—Ä–æ–±–µ–ª—ã —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–ª—é—á–µ–π
                for k in keys:
                    if pd.api.types.is_string_dtype(df[k]):
                        df[k] = df[k].str.strip()
                df = df.drop_duplicates(subset=keys)

            # --- –∑–∞–≥—Ä—É–∑–∫–∞ ---
            if not inspector.has_table(table_name):
                df.to_sql(
                    table_name,
                    con=conn,
                    index=False,
                    if_exists="replace",
                    dtype=dtype_map
                )
                print(f"üÜï {table_name} —Å–æ–∑–¥–∞–Ω–∞ –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫.")
                continue

            if keys:
                stg = f"{table_name}_stg"
                df.to_sql(stg, con=conn, index=False,
                          if_exists="replace", dtype=dtype_map)
                on_clause = " AND ".join([f"t.[{k}] = s.[{k}]" for k in keys])
                cols = [f"[{c}]" for c in df.columns]
                insert_cols = ", ".join(cols)
                insert_vals = ", ".join([f"s.[{c}]" for c in df.columns])
                merge_sql = f"""
MERGE INTO dbo.{table_name} AS t
USING dbo.{stg} AS s
ON {on_clause}
WHEN NOT MATCHED BY TARGET THEN
    INSERT ({insert_cols}) VALUES ({insert_vals});
DROP TABLE dbo.{stg};
"""
                conn.execute(text(merge_sql))
                print(f"üì• {table_name} ‚Üí –¥–æ–±–∞–≤–ª–µ–Ω—ã —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ ({len(df)} –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ).")
            else:
                df.to_sql(
                    table_name,
                    con=conn,
                    index=False,
                    if_exists="append",
                    dtype=dtype_map
                )
                print(f"‚ö†Ô∏è {table_name}: –Ω–µ—Ç –∫–ª—é—á–µ–π, –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤—Å–µ {len(df)} —Å—Ç—Ä–æ–∫.")

if __name__ == "__main__":
    DATA_URL = "https://smartup.online/b/anor/mxsx/mr/inventory$export"
    cookies = get_cookies_from_browser("https://smartup.online")
    result = fetch_inventory(DATA_URL, cookies)
    if result:
        upload_to_sql(result)
